\section{(Appendix): Relevant functions and Identities}\label{s:appendices}
We define the (Euler-)Gamma function as follows:
\begin{definition}\label{d: eg}
    for \(\Re(z) > 0\), we have the following: \(\Gamma(z) = \int_{0}^{\infty} t^{z-1} e^{-t} dt\)
\end{definition}

The Gamma function can be seen as an extension of the factorial function, for non-integers. This function is defined for complex numbers and all there subsets (so also real numbers), as long as the condition above holds. For positive integers values \(z\), we have the following identity: \(\Gamma(z) = (z - 1)!\)
Other important identities, not necessarily for \(z\) an integer, are: 
\begin{itemize}
    \item \(\Gamma(z + 1) = z \Gamma(z)\)
    \item \(\Gamma(2) = \Gamma(1) = 1\)
    \item \(\Gamma(\frac{1}{2}) = \sqrt{\pi}\)
\end{itemize}

\begin{definition}\label{d: ff}
    The falling factorial is defined as follows: \((x)_n = \prod_{k = 0}^{n - 1} (x - k)\), which is a polynomial
\end{definition}
\begin{definition}
    For \(0 \leq k \leq n\), the Binomial Coefficient is defined as follows: \(\binom{n}{k}\), where \(n, k \in \mathbb{N}\).
\end{definition}
We can derive the following factorial identity, which is convenient to work with analytically: \(\binom{n}{k} = \frac{n!}{k! (n - k)!}\).
For numerically computing expressions containing the Binomial Coefficient, the following identity is computationally more efficient: \(\binom{n}{k} = \frac{(n)_k}{k!}\). With \((n)_k\) as in \ref{d: ff}.
Since we have established in \ref{d: eg} that \(\Gamma(z) = (z - 1)!\), we can rewrite our factorial identify to:
\[\binom{n}{k} = \frac{\Gamma(n + 1)}{\Gamma(k + 1) \cdot \Gamma( n - k  + 1)} = \frac{n}{k} \cdot\frac{\Gamma(n)}{\Gamma(k) \cdot \Gamma(n - k + 1)}\]

\begin{definition}
    Vandermonde's identity: for non-negative integers, \(k, l, m, n\), we have that \[\sum_{k = 0}^{l} \binom{m}{k} \cdot \binom{n}{l - k} = \binom{m + n}{l}\].
\end{definition}
A modification on the latter identity has been called the Chu-Vandermonde identity. This is the same identity, but it his been proven that the identities still hold for complex values \(m, n\) as long as \(l\) is a positive integer (\cite{askey75}).

For the interchange-ability of derivatives and integrals and sums, we can apply the following two theorems:
\begin{comment}
\begin{theorem}
    Leibnitz's Rule:
    Let \(f(x, \theta), a(\theta), b(\theta)\) be differentiable with respect to \(\theta\), then we have that:
    \[ \frac{d}{d\theta} \int_{a(\theta)}^{b(\theta)} f(x, \theta) dx = f(b(\theta), \theta) \frac{d b(\theta)}{d\theta} - f(a(\theta), \theta) \frac{d a(\theta)}{d\theta} + \int_{a(\theta)}^{b(\theta)} \frac{\partial f(x, \theta) }{\partial \theta} dx.\] For the special case, where \(a(\theta), b(\theta)\) are constant we have that: 
    \[\frac{d}{d\theta} \int_{a}^{b} f(x, \theta) dx =  \int_{a}^{b} \frac{ \partial f(x, \theta)}{ \partial d\theta}.\]
\end{theorem}

For the interchange-ability of derivatives and summations, the following theorem has been given by \cite{casella2002}:
\begin{theorem}
    Suppose that the series \(\sum_{x = 0}^{\infty} h(\theta, x)\) converges for all \(\theta\) in an interval \((a, b)\) of real numbers and 
    
    \begin{enumerate}[(i)]
        \item \(\frac{\partial h(\theta, x)}{\partial \theta}\) is continuous for all \(x\)
        \item \(\sum_{x = 0}^{\infty} \frac{\partial h(\theta, x)}{\partial \theta}\) converges uniformly on every closed bounded subinterval of \((a, b)\)
    \end{enumerate}
    Then:
    \[
    \frac{d}{d \theta} \left( \sum_{x = 0}^{\infty} h(\theta, x) \right) = \sum_{x = 0}^{\infty} \frac{\partial h(\theta, x)}{\partial \theta}
    \]
    
   
\end{theorem}
\end{comment}
\section{(Appendix): Proofs}\label{s:app_B}

\subsection{Proofs section \ref{s:calculus}}

\begin{proof}
    Proof of \ref{p: calculus}
    \begin{enumerate}[(i)]
        \item We will proof for the Riemann-Liouville derivative, the proof for the Caputo-Fabrizio derivative is very similar and the Grünwald-Letnikov derivative is a direct consequence of the linearity of the sum.
        \[ D^{\alpha} (a f(x) + b g(x)) = \frac{d^n}{dx^n}\frac{1}{\Gamma(n - \alpha)} \int_{0}^{x} (x - t)^{n - \alpha - 1} (a f(t) + b g(t)) dt \]
     
        \[= \frac{d^n}{dx^n}\left(\frac{a}{\Gamma(n - \alpha)} \int_{0}^{x} (x - t)^{n - \alpha - 1}  f(t)dt + \frac{b}{\Gamma(n - \alpha)} \int_{0}^{x} (x - t)^{n - \alpha - 1} g(t)dt\right) \] Where we simply split the integral and put the constants in front.
        \[= \frac{d^n}{dx^n}\frac{a}{\Gamma(n - \alpha)} \int_{0}^{x} (x - t)^{n - \alpha - 1}  f(t)dt + \frac{d^n}{dx^n} \frac{b}{\Gamma(n - \alpha)} \int_{0}^{x} (x - t)^{n - \alpha - 1} g(t)dt \] As the regular derivative operator is linear.
        \[ = a D^{\alpha} f(x) + b D^{\alpha} g(x) \]
        \item Intuitively, this makes perfect sense, as the 0-th derivative is just no derivative, so just the function \(f(x)\). But for these derivatives, a little bit more effort is needed to prove this rather obvious fact.
        \newline 
        For the Grünwald-Letnikov derivative we get: \[D^0 f(x) = \lim_{h \to 0} \frac{1}{h^0} \sum_{k=0}^\infty (-1)^k \binom{0}{k} f(x - k h)
        = \lim_{h \to 0} \frac{1}{1} \sum_{k=0}^\infty (-1)^k \frac{0!}{k!(0- k)!} f(x - k h).\] The factorial Identity of the binomial coefficient only holds for \(0 \leq k \leq \alpha\). Since \(\alpha = 0\) and k is always a positive integer lesser or equal to \(\alpha, k = 0\). Thus, we get:
        \[ = \lim_{h \to 0} \sum_{k=0}^\infty (-1)^0 \frac{0!}{0!(0- 0)!} f(x - 0 h) = \lim_{h \to 0} f(x - 0 h) = f(x).\]
        \newline
        For the Caputo-Fabrizio derivative, we obtain the following:
        \[ D^{0} f(x) = \frac{1}{1 - 0}  \int_{0}^{x} \exp\left(\frac{0}{1 -0}(x-t)\right) f'(t) dt = \int_{0}^{x}f'(t) dt = f(x).\]
        \newline
        Finally, for the Riemann-Liouville derivative, we can simply make use of \autoref{d: differintegral} and \autoref{r: integer} to note that in this context \(\alpha = 0\) is included in the natural integers. So \(D^\alpha = \frac{d^\alpha}{dx^\alpha} f(x) = {d^0}{dx^0} f(x) = f(x)\) by the first fundamental theorem of calculus.

        \item The proof for the Riemann-Liouville derivative is given by \cite{koning15}. And the proof for the Caputo-Fabrizio derivative is given by \cite{losada15}.For the Grünwald-Letnikov derivative, we get:
        \[ D^\alpha(D^\beta f(x)) = \lim_{h \to 0} \frac{1}{h^\alpha} \sum_{k=0}^\infty (-1)^k \binom{\alpha}{k}(  \frac{1}{h^\beta} \sum_{l=0}^\infty (-1)^l \binom{\beta}{l} f(x - l h - kh))\]
        \[= \lim_{h \to 0} \frac{1}{h^{\alpha + \beta}} \sum_{k=0}^\infty (-1)^k \binom{\alpha}{k} \sum_{l=0}^\infty (-1)^l \binom{\beta}{l} f(x - (k + l)h).\] We substitute \(m = k + l\) to deal with the dubble sums: 
        \[ \lim_{h \to 0} \frac{1}{h^{\alpha + \beta}} \sum_{m=0}^\infty f(x - mh)  \sum_{k=0}^m (-1)^k (-1)^{ m - k} \binom{\alpha}{k} \binom{\beta}{m - k}\] Now we make use of an identify from \autoref{s:appendices} to obtain:
        \[ = \lim_{h \to 0} \frac{1}{h^{\alpha + \beta}} \sum_{m=0}^\infty (-1)^m \binom{\alpha + \beta}{m} f(x - mh) = D^{\alpha + \beta} f(x).\]
        It can be shown in an exactly similar way that the latter expression is equal to \(D^\beta(D^\alpha f(x))\).

        
    \end{enumerate}
\end{proof}
\begin{comment}
\subsection{Proofs section \ref{s: moments}}

\begin{proof}
    Theorem \ref{t: negative}:
Suppose for the moment that \( X \) is a positive random variable. Since \( x f(x) \) is integrable for \( x > 0 \), we have:

\[
E(X) = \int_0^\infty x \, dF(x) = \int_0^\infty \int_0^\infty e^{tx} \, dt \, dF(x).
\]

We can interchange the order of integration as follows:

\[
E(X) = \int_0^\infty e^{tx} \, dF(x) \, dt = \int_0^\infty M_X(-t) \, dt.
\]

The interchange of the order of integration is subject to \( E(e^{-tX}) \) being integrable from \( t = 0 \) to \( t = \infty \).

Finally, by substituting \( X^{-1} \) for \( X \), we find:

\[
E(X^{-1}) = \int_0^\infty M_X^{-1}(-t) \, dt.
\]

There are two natural ways to generalize (1) to \( E(X^{-1}) \); one way gives:

\[
E(X^{-n}) = \int_0^\infty \int_0^\infty \cdots \int_0^\infty M_X(-t_n) \, dt_n \cdots dt_2 dt_1, \tag{2}
\]

while the second way gives:

\[
E(X^{-n}) = \frac{1}{\Gamma(n)} \int_0^\infty t^{n-1} M_X(-t) \, dt.
\]
\cite{cressie1981}
\end{proof}
\end{comment}

\section{Appendix C }\label{s: app_C}
TODO: ADD TABLE OF COMMON DISTRIBUTIONS + SUPPORT, MGF, ETC.
